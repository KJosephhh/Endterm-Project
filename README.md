LLama Chat is a simple web app where you can talk to the LLama model using the Ollama API. You type a question, and it sends it to the model to generate a response. The response shows up in real time, right after you send your message. The foundation of this project comes from experimenting with the Ollama application, which allows users to install and run AI models locally on their personal computers. However Ollama doesn't offer any built in user interface. So the speed of the models responses may vary among hardware. 

REQUIREMENTS:
This program requires [Ollama](https://github.com/ollama/ollama) and llama3.2 in order for the AI model to function properly along with the Ionide-fsharp extension. 

1. install Ollama and run it.
2. Make sure you run the ```<copy-button>ollama run llama3.2</copy-button>```
3. CD into the project folder and use the ```<copy-button>dotnet run</copy-button>``` command.
4. You're done.


Site Preview
https://kjosephhh.github.io/Midterm-Project/
